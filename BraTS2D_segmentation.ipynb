{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31dce90a",
   "metadata": {},
   "source": [
    "# BraTS2020 **2D** Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec90921",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Loading data and preprocessing\n",
    "### 1.1 Loading data and preprocessing\n",
    "**Pipeline**\n",
    "1. Group all `.h5` slices by patient (`volume_<vid>_slice_<sid>.h5`)\n",
    "2. Patient-wise subset & train/val/test split (avoids leakage)\n",
    "3. 2D preprocessing: drop all-zero masks, percentile normalization, center pad/crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c76c6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 8\n",
      "Using 8 data loader workers\n",
      "Using device: cuda\n",
      "Reading slices from: /home/tanhuoyuan_gmail_com/DIT698_DML_Project_BraTS_Hightlight/brats2020-training-data/BraTS2020_training_data/content/data\n"
     ]
    }
   ],
   "source": [
    "import os, re, random, h5py, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "import json\n",
    "\n",
    "DATA_ROOT = Path('./brats2020-training-data/BraTS2020_training_data/content/data')\n",
    "OUTPUT_DIR = Path('./outputs'); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBSET_FRACTION = 1.0 #0.02 # 1.0 # 0.5\n",
    "SEED = 42\n",
    "BATCH_SIZE = 192 # 96 # 64 # 128\n",
    "EPOCHS = 60 # 80\n",
    "\n",
    "ES_PATIENCE = 10\n",
    "ES_MIN_DELTA = 1e-4\n",
    "\n",
    "print(f'CPU count: {os.cpu_count()}')\n",
    "num_workers = min(os.cpu_count(),  12)\n",
    "print(f'Using {num_workers} data loader workers')\n",
    "# num_workers = 0  # set >0 after debugging\n",
    "\n",
    "LR = 1e-4\n",
    "IMG_SIZE = (240, 240)\n",
    "# DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "print(f'Using device: {DEVICE}')\n",
    "print(f'Reading slices from: {DATA_ROOT.resolve()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef82080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 369 patients. Example IDs: [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Total slices discovered: 57195\n"
     ]
    }
   ],
   "source": [
    "H5_RE = re.compile(r'^volume_(\\d+)_slice_(\\d+)\\.h5$', re.IGNORECASE)\n",
    "\n",
    "def group_by_volume(root: Path):\n",
    "    groups = defaultdict(list)\n",
    "    for p in root.glob('*.h5'):\n",
    "        m = H5_RE.fullmatch(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        vid = int(m.group(1)); sid = int(m.group(2))\n",
    "        groups[vid].append((sid, p.resolve()))\n",
    "    grouped = {vid: [path for sid, path in sorted(items, key=lambda x: x[0])]\n",
    "               for vid, items in sorted(groups.items(), key=lambda kv: kv[0])}\n",
    "    return grouped\n",
    "\n",
    "groups = group_by_volume(DATA_ROOT)\n",
    "print(f'Found {len(groups)} patients. Example IDs:', list(groups.keys())[:8])\n",
    "total_slices = sum(len(v) for v in groups.values())\n",
    "print(f'Total slices discovered: {total_slices}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca0a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset patients: 369 | train: 236, val: 59, test: 74\n",
      "Slices -> train: 36580, val: 9145, test: 11470\n"
     ]
    }
   ],
   "source": [
    "all_vids = list(groups.keys())\n",
    "random.Random(SEED).shuffle(all_vids)\n",
    "subset_n = max(1, int(len(all_vids) * SUBSET_FRACTION))\n",
    "subset_vids = sorted(all_vids[:subset_n])\n",
    "\n",
    "train_vids, test_vids = train_test_split(subset_vids, test_size=0.2, random_state=SEED)\n",
    "train_vids, val_vids  = train_test_split(train_vids, test_size=0.2, random_state=SEED)\n",
    "\n",
    "def gather_paths(vid_list):\n",
    "    out = []\n",
    "    for vid in vid_list:\n",
    "        out.extend(groups[vid])\n",
    "    return out\n",
    "\n",
    "train_paths = gather_paths(train_vids)\n",
    "val_paths   = gather_paths(val_vids)\n",
    "test_paths  = gather_paths(test_vids)\n",
    "\n",
    "print(f'Subset patients: {len(subset_vids)} | train: {len(train_vids)}, val: {len(val_vids)}, test: {len(test_vids)}')\n",
    "print(f'Slices -> train: {len(train_paths)}, val: {len(val_paths)}, test: {len(test_paths)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32a7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, numpy as np\n",
    "\n",
    "def read_h5(path: Path):\n",
    "    \"\"\"\n",
    "    Expect:\n",
    "      image: (H, W, 4) or (4, H, W)  -> returns (4, H, W) float32\n",
    "      mask:  (H, W, 3) one-hot for (Necrosis, Edema, Enhancing)\n",
    "             -> returns (H, W) int64 with labels: 0=BG, 1=Necrosis, 2=Edema, 3=Enhancing\n",
    "    \"\"\"\n",
    "    # with h5py.File(path, \"r\") as f:\n",
    "    with h5py.File(path, \"r\", libver=\"latest\", swmr=True) as f:\n",
    "        img = f[\"image\"][()]\n",
    "        # to channels-first (4, H, W)\n",
    "        if img.ndim == 3 and img.shape[-1] in (3,4):\n",
    "            img = np.moveaxis(img, -1, 0)\n",
    "        elif img.ndim == 2:\n",
    "            img = img[None, ...]\n",
    "        elif img.ndim == 3 and img.shape[0] in (3,4):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected image shape: {img.shape}\")\n",
    "\n",
    "        m = f[\"mask\"][()]  # (H, W, 3) one-hot\n",
    "        if m.ndim != 3 or m.shape[-1] != 3:\n",
    "            raise ValueError(f\"Expect mask (H,W,3) one-hot, got {m.shape}\")\n",
    "\n",
    "        # background channel = 1 - any(foreground)\n",
    "        bg = (m <= 0).all(axis=-1, keepdims=True).astype(np.uint8)  # (H,W,1)\n",
    "        # stack as [BG, Necrosis, Edema, Enhancing]\n",
    "        four = np.concatenate([bg, (m > 0).astype(np.uint8)], axis=-1)  # (H,W,4)\n",
    "        # to class indices 0..3\n",
    "        cls = np.argmax(four, axis=-1).astype(np.int64)  # (H,W)\n",
    "\n",
    "        return img.astype(np.float32), cls  # (4,H,W), (H,W) int64\n",
    "\n",
    "\n",
    "def percentile_normalize(img, lower=0.5, upper=99.5, eps=1e-6):\n",
    "    # img: (C,H,W)\n",
    "    out = np.empty_like(img, dtype=np.float32)\n",
    "    for c in range(img.shape[0]):\n",
    "        lo = np.percentile(img[c], lower); hi = np.percentile(img[c], upper)\n",
    "        out[c] = np.clip((img[c]-lo)/(hi-lo+eps), 0, 1)\n",
    "    return out\n",
    "\n",
    "def _pad2d_or_3d(arr, ph, pw):\n",
    "    if arr.ndim == 3:  # (C,H,W)\n",
    "        return np.pad(arr, ((0,0),(ph//2, ph-ph//2),(pw//2, pw-pw//2)), mode=\"constant\")\n",
    "    if arr.ndim == 2:  # (H,W)\n",
    "        return np.pad(arr, ((ph//2, ph-ph//2),(pw//2, pw-pw//2)), mode=\"constant\")\n",
    "    raise ValueError(f\"Unsupported ndim={arr.ndim}\")\n",
    "\n",
    "def center_pad_crop(img, mask, size=(240,240)):\n",
    "    H, W = img.shape[1], img.shape[2]; th, tw = size\n",
    "    ph, pw = max(0, th-H), max(0, tw-W)\n",
    "    if ph>0 or pw>0:\n",
    "        img  = _pad2d_or_3d(img,  ph, pw)\n",
    "        mask = _pad2d_or_3d(mask, ph, pw)\n",
    "    H2, W2 = img.shape[1], img.shape[2]\n",
    "    sy, sx = (H2-th)//2, (W2-tw)//2\n",
    "    return img[:, sy:sy+th, sx:sx+tw], mask[sy:sy+th, sx:sx+tw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923c547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BraTS2DDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Lazy-loading dataset:\n",
    "      - Keep only file paths in __init__\n",
    "      - Read/normalize/pad-crop on-the-fly in __getitem__\n",
    "      - Optionally drop slices whose mask is all background\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, drop_all_zero=True, size=(240, 240)):\n",
    "        self.paths = list(paths)\n",
    "        self.drop_all_zero = drop_all_zero\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def _load_one(self, p: Path):\n",
    "        # read from disk\n",
    "        img, cls = read_h5(p)                 # img: (4,H,W)  cls: (H,W) int\n",
    "        # optional drop: keep only slices with any foreground\n",
    "        if self.drop_all_zero and cls.max() == 0:\n",
    "            return None\n",
    "        # preprocessing\n",
    "        img = percentile_normalize(img)\n",
    "        img, cls = center_pad_crop(img, cls, size=self.size)\n",
    "        # to tensors\n",
    "        x = torch.from_numpy(img).float()     # (C,H,W)\n",
    "        y = torch.from_numpy(cls).long()      # (H,W)\n",
    "        return x, y, p.name.replace('.h5','')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        sample = self._load_one(p)\n",
    "\n",
    "        # If this slice was dropped due to all-zero mask, fallback to a nearby one\n",
    "        if sample is None:\n",
    "            # linear probing to find the next valid slice\n",
    "            j = (idx + 1) % len(self.paths)\n",
    "            while True:\n",
    "                sample = self._load_one(self.paths[j])\n",
    "                if sample is not None or j == idx:\n",
    "                    break\n",
    "                j = (j + 1) % len(self.paths)\n",
    "\n",
    "            # as a last resort (all zero), create a dummy empty slice (rare)\n",
    "            if sample is None:\n",
    "                img, cls = read_h5(self.paths[idx])\n",
    "                img = percentile_normalize(img)\n",
    "                img, cls = center_pad_crop(img, cls, size=self.size)\n",
    "                sample = (torch.from_numpy(img).float(),\n",
    "                          torch.from_numpy(cls).long(),\n",
    "                          self.paths[idx].name.replace('.h5',''))\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "def make_loader_kwargs(num_workers: int):\n",
    "    if num_workers == 0:\n",
    "        return dict(num_workers=0, pin_memory=False)  # single-process, no prefetch/persistent\n",
    "    else:\n",
    "        return dict(\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2,\n",
    "        )\n",
    "\n",
    "common_kwargs = make_loader_kwargs(num_workers)\n",
    "\n",
    "train_ds = BraTS2DDataset(train_paths, drop_all_zero=True,  size=IMG_SIZE)\n",
    "val_ds   = BraTS2DDataset(val_paths,   drop_all_zero=True,  size=IMG_SIZE)\n",
    "test_ds  = BraTS2DDataset(test_paths,  drop_all_zero=False, size=IMG_SIZE)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  **common_kwargs)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, **common_kwargs)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1,          shuffle=False, **common_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626babae",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2 Train/Evaluate Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a54979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def viz_overlays(\n",
    "    ds,\n",
    "    model,\n",
    "    device=\"cpu\",\n",
    "    n=6,\n",
    "    rows=None,\n",
    "    cols=None,\n",
    "    base_channel=1,                 # which modality to display as grayscale (e.g., 1 = T1Gd)\n",
    "    class_colors=None,              # dict like {1:'y', 2:'c', 3:'m'}\n",
    "    gt_linestyle=\"solid\",\n",
    "    pred_linestyle=\"dashed\",\n",
    "    linewidth=1.2,\n",
    "    figsize=(10, 6),\n",
    "    seed=None,\n",
    "):\n",
    "    if class_colors is None:\n",
    "        class_colors = {1: \"y\", 2: \"c\", 3: \"m\"}\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if rows is None or cols is None:\n",
    "        rows = rows or 2\n",
    "        cols = cols or int(np.ceil(n / rows))\n",
    "\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(n, len(axes))):\n",
    "            idx = np.random.randint(len(ds))\n",
    "            x, y, name = ds[idx]                # x: (C,H,W), y: (H,W)\n",
    "            xb = x[None].to(device)             # (1,C,H,W)\n",
    "\n",
    "            logits = model(xb)                  # (1,4,H,W)\n",
    "            pred = torch.argmax(torch.softmax(logits, dim=1), dim=1)[0].cpu().numpy()\n",
    "            gt   = y.cpu().numpy() if torch.is_tensor(y) else y\n",
    "\n",
    "            # choose background channel safely\n",
    "            bch = base_channel\n",
    "            if bch >= x.shape[0]:\n",
    "                bch = 0\n",
    "            base = x.numpy()[bch] if torch.is_tensor(x) else x[bch]\n",
    "\n",
    "            ax = axes[i]\n",
    "            ax.imshow(base, cmap=\"gray\")\n",
    "\n",
    "            # draw GT (solid) and Pred (dashed) for each foreground class\n",
    "            for c in (1, 2, 3):\n",
    "                ax.contour((gt==c).astype(np.uint8),   levels=[0.5],\n",
    "                           colors=class_colors.get(c, \"w\"), linewidths=linewidth,\n",
    "                           linestyles=gt_linestyle)\n",
    "                ax.contour((pred==c).astype(np.uint8), levels=[0.5],\n",
    "                           colors=class_colors.get(c, \"w\"), linewidths=linewidth,\n",
    "                           linestyles=pred_linestyle)\n",
    "\n",
    "            ax.set_title(str(name))\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Solid = GT  |  Dashed = Pred  (1: Necrosis, 2: Edema, 3: Enhancing)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d91c4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def viz_overlays_pro(\n",
    "    ds,\n",
    "    model,\n",
    "    device=\"cpu\",\n",
    "    n=6,\n",
    "    samples_per_row=3,            # how many cases per row (each case = 1 or 2 panels)\n",
    "    base_channel=1,               # modality to display as grayscale\n",
    "    show_error=False,             # also show TP/FP/FN\n",
    "    panel_size=3.2,               # inches per panel (width ~= height)\n",
    "    class_names={1:\"Necrosis\", 2:\"Edema\", 3:\"Enhancing\"},\n",
    "    class_colors={1:\"#ffcc00\", 2:\"#00c5d8\", 3:\"#cc66ff\"},  # GT fill colors\n",
    "    pred_edge_colors=None,        # None -> reuse fill colors\n",
    "    gt_alpha=0.35,\n",
    "    pred_lw=2.6,\n",
    "    pred_ls=\"dashed\",\n",
    "    seed=None,\n",
    "    suptitle=\"GT = filled  |  Pred = dashed outline\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compact overlay visualization for multiclass segmentation.\n",
    "      - 0 = background, 1..3 = foreground classes.\n",
    "      - If `show_error=True`, each sample occupies two panels: [overlay | error].\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    if pred_edge_colors is None:\n",
    "        pred_edge_colors = {k: class_colors[k] for k in class_colors}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # how many panels per sample\n",
    "    panels_per_sample = 2 if show_error else 1\n",
    "\n",
    "    # grid sizes\n",
    "    rows = int(np.ceil(n / samples_per_row))\n",
    "    cols = samples_per_row * panels_per_sample\n",
    "\n",
    "    # figure size (proportional to number of panels)\n",
    "    fig_w = panel_size * cols\n",
    "    fig_h = panel_size * rows\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(fig_w, fig_h))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    axes = axes.reshape(rows, cols)\n",
    "\n",
    "    # helper: percentile normalization for the grayscale background\n",
    "    def _norm(x, lo=1.0, hi=99.0, eps=1e-6):\n",
    "        p1, p99 = np.percentile(x, lo), np.percentile(x, hi)\n",
    "        x = (x - p1) / (p99 - p1 + eps)\n",
    "        return np.clip(x, 0, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(n):\n",
    "            r = i // samples_per_row\n",
    "            c0 = (i % samples_per_row) * panels_per_sample\n",
    "            ax = axes[r, c0]\n",
    "\n",
    "            # pick a random sample\n",
    "            x, y, name = ds[np.random.randint(len(ds))]\n",
    "            xb = x[None].to(device)\n",
    "            logits = model(xb)\n",
    "            pred = torch.argmax(logits, dim=1)[0].cpu().numpy()\n",
    "            gt   = y.cpu().numpy() if torch.is_tensor(y) else y\n",
    "\n",
    "            # choose background modality & normalize for better contrast\n",
    "            bch  = base_channel if base_channel < x.shape[0] else 0\n",
    "            base = x.numpy()[bch] if torch.is_tensor(x) else x[bch]\n",
    "            base = _norm(base)\n",
    "\n",
    "            # ---- Overlay panel\n",
    "            ax.imshow(base, cmap=\"gray\", interpolation=\"nearest\")\n",
    "            for cc in (1, 2, 3):\n",
    "                gt_mask = (gt == cc).astype(np.uint8)\n",
    "                if gt_mask.any():\n",
    "                    ax.contourf(gt_mask, levels=[0.5, 1.5],\n",
    "                                colors=[class_colors[cc]], alpha=gt_alpha, antialiased=False)\n",
    "                pred_mask = (pred == cc).astype(np.uint8)\n",
    "                if pred_mask.any():\n",
    "                    ax.contour(pred_mask, levels=[0.5],\n",
    "                               colors=[pred_edge_colors[cc]],\n",
    "                               linewidths=pred_lw, linestyles=pred_ls, antialiased=False)\n",
    "            ax.set_title(str(name), fontsize=10)\n",
    "            ax.set_axis_off()\n",
    "            ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "            # ---- Error panel (optional)\n",
    "            if show_error:\n",
    "                ax2 = axes[r, c0 + 1]\n",
    "                tp = (pred == gt) & (gt > 0)              # green\n",
    "                fp = (pred > 0) & (gt == 0)               # red\n",
    "                fn = (pred == 0) & (gt > 0)               # blue\n",
    "                err = np.zeros((*gt.shape, 3), dtype=np.float32)\n",
    "                err[tp] = (0.0, 0.9, 0.0)\n",
    "                err[fp] = (1.0, 0.1, 0.1)\n",
    "                err[fn] = (0.1, 0.6, 1.0)\n",
    "                ax2.imshow(base, cmap=\"gray\", interpolation=\"nearest\")\n",
    "                ax2.imshow(err, alpha=0.45, interpolation=\"nearest\")\n",
    "                ax2.set_title(\"Error (TP/FP/FN)\", fontsize=10)\n",
    "                ax2.set_axis_off()\n",
    "                ax2.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    # tighten spacing; leave a small room for legend/suptitle\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.15)\n",
    "\n",
    "    # single legend for the whole figure\n",
    "    legend = []\n",
    "    for c, nm in class_names.items():\n",
    "        legend.append(Patch(facecolor=class_colors[c], edgecolor=\"none\", alpha=gt_alpha, label=f\"GT {nm}\"))\n",
    "        legend.append(Line2D([0],[0], color=pred_edge_colors[c], lw=pred_lw, linestyle=pred_ls, label=f\"Pred {nm}\"))\n",
    "    if show_error:\n",
    "        legend += [\n",
    "            Patch(facecolor=(0,0.9,0), edgecolor='none', alpha=0.6, label=\"TP\"),\n",
    "            Patch(facecolor=(1,0.1,0.1), edgecolor='none', alpha=0.6, label=\"FP\"),\n",
    "            Patch(facecolor=(0.1,0.6,1), edgecolor='none', alpha=0.6, label=\"FN\"),\n",
    "        ]\n",
    "    fig.legend(handles=legend, loc=\"upper center\", ncol=5 if show_error else 3,\n",
    "               bbox_to_anchor=(0.5, 1.02), frameon=False, fontsize=9)\n",
    "    fig.suptitle(suptitle, y=1.06, fontsize=11)\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.98))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21d0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n",
    "def dice_iou_binary(p, t, eps=1e-6):\n",
    "    inter = (p & t).sum()\n",
    "    union = (p | t).sum()\n",
    "    dice = (2*inter + eps) / (p.sum() + t.sum() + eps)\n",
    "    iou  = (inter + eps) / (union + eps)\n",
    "    return float(dice), float(iou)\n",
    "\n",
    "def evaluate_multiclass(\n",
    "    loader,\n",
    "    model,\n",
    "    device=\"cpu\",\n",
    "    num_classes=4,\n",
    "    add_overall_mean: bool = True,\n",
    "    overall_label: str = \"Overall_Mean\",\n",
    "    save_csv_dir = None,                  # e.g., OUTPUT_DIR Path; saves 2 CSVs if not None\n",
    "    model_name=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a multiclass model on a DataLoader.\n",
    "\n",
    "    Returns:\n",
    "      per_slice_df : metrics per slice\n",
    "      per_vol_df   : metrics aggregated per volume (with an extra 'Overall_Mean' row if add_overall_mean=True)\n",
    "      overall_row  : dict of overall mean metrics (or None if add_overall_mean=False)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, name in loader:\n",
    "            x = x.to(device)        # (1, C, H, W)\n",
    "            y = y.to(device)        # (1, H, W) long\n",
    "            logits = model(x)       # (1, num_classes, H, W)\n",
    "\n",
    "            pred = torch.argmax(torch.softmax(logits, dim=1), dim=1)[0].cpu().numpy()\n",
    "            gt   = y[0].cpu().numpy()\n",
    "\n",
    "            metrics = {\"id\": name[0]}\n",
    "            dices, ious = [], []\n",
    "            for c, cname in zip([1, 2, 3], [\"Necrosis\", \"Edema\", \"Enhancing\"]):\n",
    "                p = (pred == c).astype(np.uint8)\n",
    "                t = (gt   == c).astype(np.uint8)\n",
    "                d, i = dice_iou_binary(p, t)\n",
    "                metrics[f\"dice_{cname}\"] = d\n",
    "                metrics[f\"iou_{cname}\"]  = i\n",
    "                dices.append(d); ious.append(i)\n",
    "\n",
    "            metrics[\"dice_FG_mean\"] = float(np.mean(dices))\n",
    "            metrics[\"iou_FG_mean\"]  = float(np.mean(ious))\n",
    "            rows.append(metrics)\n",
    "\n",
    "    per_slice_df = pd.DataFrame(rows)\n",
    "\n",
    "    # ---- aggregate per volume\n",
    "    def volid(s):\n",
    "        m = re.match(r\"(volume_\\d+)_slice_\\d+$\", s)\n",
    "        return m.group(1) if m else s\n",
    "\n",
    "    per_slice_df[\"volume_id\"] = per_slice_df[\"id\"].map(volid)\n",
    "    per_vol_df = per_slice_df.groupby(\"volume_id\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    overall_row = None\n",
    "    if add_overall_mean and len(per_vol_df) > 0:\n",
    "        # compute overall mean across all numeric columns\n",
    "        overall_row = per_vol_df.mean(numeric_only=True).to_dict()\n",
    "        overall_row[\"volume_id\"] = overall_label\n",
    "        # append as the last row\n",
    "        per_vol_df = pd.concat([per_vol_df, pd.DataFrame([overall_row])], ignore_index=True)\n",
    "\n",
    "    # optional: save CSVs\n",
    "    if save_csv_dir is not None:\n",
    "        (save_csv_dir / f\"{model_name}_metrics_mc_per_slice.csv\").write_text(per_slice_df.to_csv(index=False))\n",
    "        (save_csv_dir / f\"{model_name}_metrics_mc_per_volume.csv\").write_text(per_vol_df.to_csv(index=False))\n",
    "\n",
    "    return per_slice_df, per_vol_df, overall_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c467719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def viz_compare(val_ds, model1, model2, device=\"cpu\", n=6, model1_name=None, model2_name=None):\n",
    "    \"\"\"\n",
    "    Visualize UNet vs Encoder–Decoder on the same slices.\n",
    "    GT: solid contours, UNet: dashed, EncDec: dotted.\n",
    "    \"\"\"\n",
    "    model1.eval(); model2.eval()\n",
    "    rows, cols = (n + 2)//3, 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 4*rows))\n",
    "    axes = axes.ravel()\n",
    "    colors = {1:\"y\", 2:\"c\", 3:\"m\"}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(n, len(axes))):\n",
    "            idx = np.random.randint(len(val_ds))\n",
    "            x, y, name = val_ds[idx]\n",
    "            base_img = x.numpy()[1] if x.shape[0] > 1 else x.numpy()[0]\n",
    "\n",
    "            xb = x[None].to(device)\n",
    "            pu = torch.argmax(torch.softmax(model1(xb),  dim=1), dim=1)[0].cpu().numpy()\n",
    "            pe = torch.argmax(torch.softmax(model2(xb),    dim=1), dim=1)[0].cpu().numpy()\n",
    "            gt = y.numpy()\n",
    "\n",
    "            ax = axes[i]; ax.imshow(base_img, cmap=\"gray\")\n",
    "            for c in (1,2,3):\n",
    "                ax.contour((gt==c).astype(np.uint8), levels=[0.5], colors=colors[c], linewidths=1.2)                 # GT (solid)\n",
    "                ax.contour((pu==c).astype(np.uint8), levels=[0.5], colors=colors[c], linewidths=1.2, linestyles=\"dashed\") # UNet\n",
    "                ax.contour((pe==c).astype(np.uint8), levels=[0.5], colors=colors[c], linewidths=1.2, linestyles=\"dotted\") # EncDec\n",
    "            ax.set_title(f\"{name} (solid=GT, dashed={model1_name}, dotted={model2_name})\")\n",
    "            ax.axis(\"off\")\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6254465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def viz_compare_triplet(\n",
    "    ds,\n",
    "    model1, model2,\n",
    "    device=\"cpu\",\n",
    "    n=6,\n",
    "    samples_per_row=3,\n",
    "    base_channel=1,\n",
    "    class_names={1:\"Necrosis\", 2:\"Edema\", 3:\"Enhancing\"},\n",
    "    # GT light fills\n",
    "    gt_colors={1:\"#ffe8a6\", 2:\"#bff6fb\", 3:\"#efd3ff\"},\n",
    "    gt_alpha=0.30,\n",
    "    gt_edge=True,\n",
    "    # UNet (model1) warm edges\n",
    "    m1_colors={1:\"#ffb000\", 2:\"#ff6f00\", 3:\"#a64cff\"},\n",
    "    # TL/EncDec (model2) cool edges\n",
    "    m2_colors={1:\"#007a99\", 2:\"#00a7b7\", 3:\"#5b6cff\"},\n",
    "    # line styles/widths\n",
    "    m1_ls=\"--\", m2_ls=\":\",\n",
    "    m1_lw=3.0, m2_lw=3.0,\n",
    "    # two-pass halo parameters\n",
    "    m1_halo_color=\"black\", m2_halo_color=\"white\",\n",
    "    m1_halo_extra=2.2, m2_halo_extra=2.2,\n",
    "    panel_size=3.0,\n",
    "    seed=None,\n",
    "    title1=\"UNet\", title2=\"TL\"\n",
    "):\n",
    "    \"\"\"\n",
    "    GT = light filled polygons (+optional thin white edge).\n",
    "    Model1 = dashed warm outline with black halo (two-pass).\n",
    "    Model2 = dotted cool outline with white halo (two-pass).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    model1.eval(); model2.eval()\n",
    "\n",
    "    rows = int(np.ceil(n / samples_per_row))\n",
    "    cols = samples_per_row\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(panel_size*cols, panel_size*rows))\n",
    "    axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "    def norm01(arr, lo=1, hi=99, eps=1e-6):\n",
    "        p1, p99 = np.percentile(arr, lo), np.percentile(arr, hi)\n",
    "        return np.clip((arr - p1) / (p99 - p1 + eps), 0, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(n, len(axes))):\n",
    "            idx = np.random.randint(len(ds))\n",
    "            x, y, name = ds[idx]                   # x:(C,H,W), y:(H,W)\n",
    "            xb = x[None].to(device)\n",
    "            p1 = torch.argmax(model1(xb), dim=1)[0].cpu().numpy()\n",
    "            p2 = torch.argmax(model2(xb), dim=1)[0].cpu().numpy()\n",
    "            gt = y.cpu().numpy() if torch.is_tensor(y) else y\n",
    "\n",
    "            ch   = base_channel if base_channel < x.shape[0] else 0\n",
    "            base = x.numpy()[ch] if torch.is_tensor(x) else x[ch]\n",
    "            base = norm01(base)\n",
    "\n",
    "            ax = axes[i]\n",
    "            ax.imshow(base, cmap=\"gray\", interpolation=\"nearest\")\n",
    "\n",
    "            for c in (1,2,3):\n",
    "                gtm = (gt==c).astype(np.uint8)\n",
    "                if gtm.any():\n",
    "                    ax.contourf(gtm, levels=[0.5,1.5],\n",
    "                                colors=[gt_colors[c]], alpha=gt_alpha, antialiased=False, zorder=1)\n",
    "                    if gt_edge:\n",
    "                        ax.contour(gtm, levels=[0.5], colors=[\"#ffffff\"],\n",
    "                                   linewidths=0.8, linestyles=\"solid\", antialiased=False, zorder=2)\n",
    "\n",
    "                # --- model1 (UNet): HALO pass (behind), then COLOR pass (front)\n",
    "                m1m = (p1==c).astype(np.uint8)\n",
    "                if m1m.any():\n",
    "                    ax.contour(m1m, levels=[0.5], colors=[m1_halo_color],\n",
    "                               linewidths=m1_lw + m1_halo_extra, linestyles=m1_ls,\n",
    "                               antialiased=False, zorder=4)   # halo\n",
    "                    ax.contour(m1m, levels=[0.5], colors=[m1_colors[c]],\n",
    "                               linewidths=m1_lw, linestyles=m1_ls,\n",
    "                               antialiased=False, zorder=5)   # colored edge\n",
    "\n",
    "                # --- model2 (TL): HALO pass (behind), then COLOR pass (front)\n",
    "                m2m = (p2==c).astype(np.uint8)\n",
    "                if m2m.any():\n",
    "                    ax.contour(m2m, levels=[0.5], colors=[m2_halo_color],\n",
    "                               linewidths=m2_lw + m2_halo_extra, linestyles=m2_ls,\n",
    "                               antialiased=False, zorder=6)   # halo\n",
    "                    ax.contour(m2m, levels=[0.5], colors=[m2_colors[c]],\n",
    "                               linewidths=m2_lw, linestyles=m2_ls,\n",
    "                               antialiased=False, zorder=7)   # colored edge\n",
    "\n",
    "            ax.set_title(str(name), fontsize=12, pad=8)\n",
    "            ax.set_axis_off(); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.03, hspace=0.18)\n",
    "\n",
    "    # row A: classes via GT color\n",
    "    palette = [Patch(facecolor=gt_colors[c], edgecolor=\"none\", alpha=gt_alpha,\n",
    "                    label=class_names[c]) for c in (1,2,3)]\n",
    "    legA = fig.legend(\n",
    "        handles=palette,                    # <--- use keyword\n",
    "        title=\"Classes (GT fill)\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.02),\n",
    "        ncol=len(palette),\n",
    "        frameon=False,\n",
    "        fontsize=10,\n",
    "        title_fontsize=10,\n",
    "    )\n",
    "    fig.add_artist(legA)\n",
    "\n",
    "    # row B: style key\n",
    "    style = [\n",
    "        Line2D([0],[0], color=\"#333\", lw=m1_lw+m1_halo_extra, ls=m1_ls, label=f\"{title1} (dashed)\"),\n",
    "        Line2D([0],[0], color=\"#999\", lw=m2_lw+m2_halo_extra, ls=m2_ls, label=f\"{title2} (dotted)\"),\n",
    "        Patch(facecolor=\"#d9d9d9\", edgecolor=\"none\", alpha=0.6, label=\"GT = filled\"),\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles=style,                      # <--- use keyword\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 0.965),\n",
    "        ncol=len(style),\n",
    "        frameon=False,\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0, 1, 0.92))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "428130f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def _percentile_norm(img, lo=1.0, hi=99.0, eps=1e-6):\n",
    "    p1, p99 = np.percentile(img, lo), np.percentile(img, hi)\n",
    "    return np.clip((img - p1) / (p99 - p1 + eps), 0, 1)\n",
    "\n",
    "def _binary_boundary(mask):\n",
    "    \"\"\"1-pixel boundary: mask XOR erode(mask). Works for uint8/0-1 arrays.\"\"\"\n",
    "    if mask.dtype != np.uint8:\n",
    "        mask = mask.astype(np.uint8)\n",
    "    # 4-neighborhood erosion\n",
    "    from scipy.ndimage import binary_erosion\n",
    "    er = binary_erosion(mask, structure=np.array([[0,1,0],[1,1,1],[0,1,0]], dtype=np.uint8), border_value=0)\n",
    "    return (mask ^ er).astype(np.uint8)\n",
    "\n",
    "def viz_compare_diagnostic(\n",
    "    ds,\n",
    "    model_1,\n",
    "    model_2,\n",
    "    device=\"cpu\",\n",
    "    n=6,\n",
    "    samples_per_row=2,\n",
    "    base_channel=1,\n",
    "    show_overlay=True,\n",
    "    show_boundaries=True,\n",
    "    show_agreement=True,\n",
    "    # colors\n",
    "    gt_fill_colors   ={1:\"#ffe9b0\", 2:\"#c7f5ff\", 3:\"#efd6ff\"},   # light fills for overlay\n",
    "    gt_edge_color    =\"#ffffff\",\n",
    "    unet_edge_colors ={1:\"#ff8a00\", 2:\"#ff4d00\", 3:\"#a64cff\"},   # warm\n",
    "    tl_edge_colors   ={1:\"#008aa6\", 2:\"#00b1c4\", 3:\"#5b6cff\"},   # cool\n",
    "    # line styles\n",
    "    unet_ls=\"--\", tl_ls=\":\", unet_lw=2.8, tl_lw=2.8,\n",
    "    gt_alpha=0.30, panel_size=3.1, seed=None,\n",
    "    model_1_name=None, model_2_name=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Triptych per slice:\n",
    "      [Overlay]   GT = filled, UNet dashed (warm), TL dotted (cool)\n",
    "      [Boundary]  Thin boundaries for GT(white), UNet(warm), TL(cool) -> shifts pop out\n",
    "      [Agreement] Per-pixel fusion map that encodes who is positive for each class\n",
    "    Assumes logits with 4 classes (0=BG, 1..3 FG) and y as (H,W) ints in [0..3].\n",
    "    \"\"\"\n",
    "    if seed is not None: np.random.seed(seed)\n",
    "    model_1.eval(); model_2.eval()\n",
    "\n",
    "    # how many columns per sample\n",
    "    cols_per_sample = int(show_overlay) + int(show_boundaries) + int(show_agreement)\n",
    "    assert cols_per_sample >= 1, \"Enable at least one of overlay/boundaries/agreement.\"\n",
    "\n",
    "    rows = int(np.ceil(n / samples_per_row))\n",
    "    cols = samples_per_row * cols_per_sample\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(panel_size*cols, panel_size*rows))\n",
    "    axes = np.atleast_2d(axes).reshape(rows, cols)\n",
    "\n",
    "    # agreement color table (legend below)\n",
    "    # label meaning (per pixel, per class):\n",
    "    # 0: none; 1: GT only; 2: UNet only; 3: TL only; 4: UNet∩TL; 5: GT∩UNet; 6: GT∩TL; 7: ALL-3\n",
    "    agree_colors = np.array([\n",
    "        [0.00,0.00,0.00],  # 0 (unused background, not shown)\n",
    "        [0.80,0.00,0.80],  # 1 GT only        -> magenta\n",
    "        [1.00,0.55,0.00],  # 2 UNet only      -> orange\n",
    "        [0.10,0.60,1.00],  # 3 TL only        -> blue\n",
    "        [0.10,0.85,0.85],  # 4 UNet & TL      -> cyan\n",
    "        [1.00,0.00,0.00],  # 5 GT & UNet      -> red\n",
    "        [0.00,1.00,0.00],  # 6 GT & TL        -> green\n",
    "        [1.00,1.00,0.00],  # 7 ALL-3          -> yellow\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for k in range(n):\n",
    "            r = k // samples_per_row\n",
    "            c0 = (k % samples_per_row) * cols_per_sample\n",
    "\n",
    "            # sample & predictions\n",
    "            x, y, name = ds[np.random.randint(len(ds))]\n",
    "            xb = x[None].to(device)\n",
    "            pu = torch.argmax(model_1(xb), dim=1)[0].cpu().numpy()\n",
    "            pt = torch.argmax(model_2(xb),   dim=1)[0].cpu().numpy()\n",
    "            gt = y.cpu().numpy() if torch.is_tensor(y) else y\n",
    "\n",
    "            # background\n",
    "            ch   = base_channel if base_channel < x.shape[0] else 0\n",
    "            base = x.numpy()[ch] if torch.is_tensor(x) else x[ch]\n",
    "            base = _percentile_norm(base)\n",
    "\n",
    "            col_cursor = c0\n",
    "\n",
    "            # ---------- Panel A: Overlay ----------\n",
    "            if show_overlay:\n",
    "                ax = axes[r, col_cursor]; col_cursor += 1\n",
    "                ax.imshow(base, cmap=\"gray\", interpolation=\"nearest\")\n",
    "                for c in (1,2,3):\n",
    "                    gtm = (gt==c).astype(np.uint8)\n",
    "                    if gtm.any():\n",
    "                        ax.contourf(gtm, levels=[0.5,1.5], colors=[gt_fill_colors[c]],\n",
    "                                    alpha=gt_alpha, antialiased=False, zorder=1)\n",
    "                        ax.contour(gtm, levels=[0.5], colors=[gt_edge_color],\n",
    "                                   linewidths=0.8, linestyles=\"solid\", antialiased=False, zorder=2)\n",
    "                    um = (pu==c).astype(np.uint8)\n",
    "                    if um.any():\n",
    "                        ax.contour(um, levels=[0.5], colors=[unet_edge_colors[c]],\n",
    "                                   linewidths=unet_lw, linestyles=unet_ls, antialiased=False, zorder=4)\n",
    "                    tm = (pt==c).astype(np.uint8)\n",
    "                    if tm.any():\n",
    "                        ax.contour(tm, levels=[0.5], colors=[tl_edge_colors[c]],\n",
    "                                   linewidths=tl_lw, linestyles=tl_ls, antialiased=False, zorder=5)\n",
    "                ax.set_title(str(name), fontsize=12, pad=8)\n",
    "                ax.set_axis_off(); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "            # ---------- Panel B: Boundaries ----------\n",
    "            if show_boundaries:\n",
    "                ax = axes[r, col_cursor]; col_cursor += 1\n",
    "                ax.imshow(base, cmap=\"gray\", interpolation=\"nearest\")\n",
    "                for c in (1,2,3):\n",
    "                    gb = _binary_boundary(gt==c)\n",
    "                    ub = _binary_boundary(pu==c)\n",
    "                    tb = _binary_boundary(pt==c)\n",
    "                    if gb.any(): ax.imshow(np.ma.masked_where(gb==0, gb), cmap=\"gray\", vmin=0, vmax=1,\n",
    "                                           alpha=0.9, interpolation=\"nearest\")  # white (see note below)\n",
    "                    if ub.any(): ax.contour(ub, levels=[0.5], colors=[unet_edge_colors[c]],\n",
    "                                            linewidths=unet_lw+1.6, linestyles=unet_ls, zorder=4)\n",
    "                    if tb.any(): ax.contour(tb, levels=[0.5], colors=[tl_edge_colors[c]],\n",
    "                                            linewidths=tl_lw+1.6, linestyles=tl_ls, zorder=5)\n",
    "                # white GT boundary via imshow above (fast, bold); if you prefer contour:\n",
    "                # ax.contour(gb, levels=[0.5], colors=[gt_edge_color], linewidths=2.0)\n",
    "                ax.set_title(\"Boundaries\", fontsize=11)\n",
    "                ax.set_axis_off(); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "            # ---------- Panel C: Agreement / Disagreement ----------\n",
    "            if show_agreement:\n",
    "                ax = axes[r, col_cursor]; col_cursor += 1\n",
    "                ax.imshow(base, cmap=\"gray\", interpolation=\"nearest\")\n",
    "                H,W = gt.shape\n",
    "                rgb = np.zeros((H,W,3), np.float32)\n",
    "                for c in (1,2,3):\n",
    "                    g = (gt==c); u=(pu==c); t=(pt==c)\n",
    "                    code = (g.astype(np.uint8)<<2) | (u.astype(np.uint8)<<1) | t.astype(np.uint8)\n",
    "                    # map non-zero codes to colors; zero stays background (transparent)\n",
    "                    visible = code>0\n",
    "                    rgb[visible] = agree_colors[code[visible]]\n",
    "                ax.imshow(rgb, alpha=0.60, interpolation=\"nearest\")\n",
    "                ax.set_title(\"Agreement map\", fontsize=11)\n",
    "                ax.set_axis_off(); ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    # layout & legends\n",
    "    plt.subplots_adjust(wspace=0.04, hspace=0.15)\n",
    "\n",
    "    # Legend 1: classes (GT fill)\n",
    "    class_handles = [Patch(facecolor=gt_fill_colors[c], edgecolor=\"none\", alpha=gt_alpha,\n",
    "                           label={1:\"Necrosis\",2:\"Edema\",3:\"Enhancing\"}[c]) for c in (1,2,3)]\n",
    "    legA = fig.legend(handles=class_handles, title=\"Classes (GT fill)\",\n",
    "                      loc=\"upper center\", bbox_to_anchor=(0.5, 1.02),\n",
    "                      ncol=3, frameon=False, fontsize=10, title_fontsize=10)\n",
    "    fig.add_artist(legA)\n",
    "\n",
    "    # Legend 2: styles for models\n",
    "    style_handles = [\n",
    "        Line2D([0],[0], color=\"#444\", lw=unet_lw, ls=unet_ls, label=f\"{model_1_name} (dashed)\"),\n",
    "        Line2D([0],[0], color=\"#888\", lw=tl_lw,   ls=tl_ls,   label=f\"{model_2_name} (dotted)\"),\n",
    "        Patch(facecolor=\"#d9d9d9\", edgecolor=\"none\", alpha=0.6, label=\"GT = filled\"),\n",
    "    ]\n",
    "    fig.legend(handles=style_handles, loc=\"upper center\", bbox_to_anchor=(0.5, 0.965),\n",
    "               ncol=3, frameon=False, fontsize=10)\n",
    "\n",
    "    # Agreement legend\n",
    "    agree_handles = [\n",
    "        Patch(color=agree_colors[1], label=\"GT only\"),\n",
    "        Patch(color=agree_colors[2], label=f\"{model_1_name} only\"),\n",
    "        Patch(color=agree_colors[3], label=f\"{model_2_name} only\"),\n",
    "        Patch(color=agree_colors[4], label=f\"{model_1_name} & {model_2_name}\"),\n",
    "        Patch(color=agree_colors[5], label=f\"GT & {model_1_name}\"),\n",
    "        Patch(color=agree_colors[6], label=f\"GT & {model_2_name}\"),\n",
    "        Patch(color=agree_colors[7], label=\"ALL-3\"),\n",
    "    ]\n",
    "    fig.legend(handles=agree_handles, loc=\"lower center\", bbox_to_anchor=(0.5, -0.02),\n",
    "               ncol=4, frameon=False, fontsize=9)\n",
    "\n",
    "    plt.tight_layout(rect=(0, 0.03, 1, 0.94))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62be53",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Loss Function\n",
    "Combined Cross-Entropy and Multi-class SoftDice Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1300a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Cross-Entropy + Multi-class SoftDice Loss\n",
    "ce = nn.CrossEntropyLoss()  # logits: (B,4,H,W); target: (B,H,W) long\n",
    "\n",
    "def soft_dice_mc(logits, target, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Multiclass soft Dice averaged over foreground classes (1..3).\n",
    "    logits: (B,C,H,W), target: (B,H,W) long in [0..C-1]\n",
    "    \"\"\"\n",
    "    probs = torch.softmax(logits, dim=1)               # (B,C,H,W)\n",
    "    onehot = torch.zeros_like(probs)                   # (B,C,H,W)\n",
    "    onehot.scatter_(1, target.unsqueeze(1), 1)         # GT one-hot\n",
    "\n",
    "    # foreground only\n",
    "    p = probs[:, 1:, :, :]                             # (B,3,H,W),foreground only\n",
    "    t = onehot[:, 1:, :, :]\n",
    "    inter = (p * t).sum(dim=(2,3)) * 2                 # (B,3)\n",
    "    den   = (p*p + t*t).sum(dim=(2,3)) + eps           # (B,3)\n",
    "    dice_per_class = inter / den                       # (B,3)\n",
    "    return 1.0 - dice_per_class.mean()                 # scalar loss\n",
    "\n",
    "def criterion(logits, target):\n",
    "    return 0.5 * ce(logits, target) + 0.5 * soft_dice_mc(logits, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33e3c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "def cleanup_cuda(model=None, optimizer=None, scheduler=None, scaler=None):\n",
    "    if model is not None:\n",
    "        try: model.to(\"cpu\")\n",
    "        except: pass\n",
    "    del model, optimizer, scheduler, scaler\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3462d9d",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.5 Train Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e52680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- metric: foreground macro-Dice (classes 1..C-1) ----------\n",
    "@torch.no_grad()\n",
    "def fg_macro_dice_from_logits(logits, y, num_classes=4, eps=1e-6):\n",
    "    pred = torch.argmax(logits, dim=1)          # (B,H,W)\n",
    "    dices = []\n",
    "    for c in range(1, num_classes):\n",
    "        p = (pred == c).float()\n",
    "        t = (y    == c).float()\n",
    "        inter = (p * t).sum(dim=(1,2)) * 2\n",
    "        den   = (p * p + t * t).sum(dim=(1,2)) + eps\n",
    "        dices.append(inter / den)\n",
    "    return torch.stack(dices, 0).mean().item()\n",
    "\n",
    "# ---------- early stopping ----------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, mode='min', patience=5, min_delta=0.0, ckpt_path=None, verbose=True):\n",
    "        assert mode in ('min', 'max')\n",
    "        self.mode, self.patience, self.min_delta = mode, patience, min_delta\n",
    "        self.verbose, self.ckpt_path = verbose, ckpt_path\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        self.num_bad, self.should_stop = 0, False\n",
    "        self.is_better = (lambda cur, best: (best - cur) > min_delta) if mode=='min' \\\n",
    "                         else (lambda cur, best: (cur - best) > min_delta)\n",
    "\n",
    "    def step(self, value, model=None):\n",
    "        if self.is_better(value, self.best):\n",
    "            self.best, self.num_bad = value, 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] best {self.mode}: {self.best:.6f}\")\n",
    "            if model is not None and self.ckpt_path is not None:\n",
    "                torch.save(model.state_dict(), self.ckpt_path)\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] no improvement ({self.num_bad}/{self.patience})\")\n",
    "            if self.num_bad >= self.patience:\n",
    "                self.should_stop = True\n",
    "        return self.should_stop\n",
    "\n",
    "# ---------- one epoch train ----------\n",
    "def train_one_epoch(\n",
    "    model, loader, optimizer, criterion, device, scaler,\n",
    "    scheduler=None, scheduler_step='batch',    # 'batch' | 'epoch' | 'plateau'\n",
    "    use_amp=True\n",
    "):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if scheduler is not None and scheduler_step == 'batch':\n",
    "            scheduler.step()\n",
    "\n",
    "        total += loss.item() * x.size(0)\n",
    "        n += x.size(0)\n",
    "\n",
    "    if scheduler is not None and scheduler_step == 'epoch':\n",
    "        scheduler.step()\n",
    "\n",
    "    return total / max(1, n)\n",
    "\n",
    "# ---------- validation ----------\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device, num_classes=4, use_amp=False):\n",
    "    model.eval()\n",
    "    total_loss, total_dice, n = 0.0, 0.0, 0\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_dice += fg_macro_dice_from_logits(logits, y, num_classes=num_classes) * x.size(0)\n",
    "        n += x.size(0)\n",
    "    return total_loss / max(1, n), total_dice / max(1, n)\n",
    "\n",
    "# ---------- high-level fit ----------\n",
    "def fit(\n",
    "    model, train_loader, val_loader,\n",
    "    criterion, optimizer, device,\n",
    "    epochs,\n",
    "    scheduler=None, scheduler_step='batch',      # 'batch' | 'epoch' | 'plateau'\n",
    "    use_amp=None,\n",
    "    early_stopping=None,                         # instance of EarlyStopping or None\n",
    "    ckpt_path=None, history_path=None, num_classes=4, model_name=\"Model\"\n",
    "):\n",
    "    if use_amp is None:\n",
    "        use_amp = (device.type == \"cuda\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"val_dice\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        tr = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, scaler,\n",
    "            scheduler=scheduler, scheduler_step=scheduler_step, use_amp=use_amp\n",
    "        )\n",
    "        va, vd = validate(model, val_loader, criterion, device,\n",
    "                          num_classes=num_classes, use_amp=False)\n",
    "\n",
    "        # ReduceLROnPlateau: step on val metric\n",
    "        if scheduler is not None and scheduler_step == 'plateau':\n",
    "            # monitor val_loss\n",
    "            scheduler.step(va)\n",
    "\n",
    "        history[\"epoch\"].append(ep)\n",
    "        history[\"train_loss\"].append(tr)\n",
    "        history[\"val_loss\"].append(va)\n",
    "        history[\"val_dice\"].append(vd)\n",
    "\n",
    "        print(f\"[{model_name} {ep:03d}] train={tr:.4f}  val={va:.4f}  dice={vd:.4f}\")\n",
    "\n",
    "        # early stopping, save best model\n",
    "        if early_stopping is not None:\n",
    "            stop = early_stopping.step(va, model=model)   # Monitor val_loss; if want val_dice, change here\n",
    "            if stop:\n",
    "                print(f\"[EarlyStopping] stop at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        # Save history after each epoch\n",
    "        if history_path is not None:\n",
    "            with open(history_path, \"w\") as f:\n",
    "                json.dump(history, f)\n",
    "\n",
    "    # Then load best checkpoint\n",
    "    if early_stopping is not None and early_stopping.ckpt_path is not None:\n",
    "        p = Path(early_stopping.ckpt_path)\n",
    "        if p.exists():\n",
    "            model.load_state_dict(torch.load(p, map_location=device))\n",
    "            print(f\"Loaded best checkpoint from {p}\")\n",
    "\n",
    "    if history_path is not None:\n",
    "        with open(history_path, \"w\") as f:\n",
    "            json.dump(history, f)\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15207584",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Train 2D Encode-Decode model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "595e6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "class ConvBNReLU(nn.Module):\n",
    "    \"\"\"(Conv -> BN -> ReLU) with same padding.\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, k, padding=k//2, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "class EncoderDecoder2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Plain encoder–decoder:\n",
    "      Encoder: repeated Conv blocks + MaxPool (downsampling)\n",
    "      Bottleneck: Conv blocks\n",
    "      Decoder: bilinear upsampling + Conv blocks (NO skip connections)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=4, out_ch=4, base=32):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.e1 = nn.Sequential(ConvBNReLU(in_ch, base),    ConvBNReLU(base, base))\n",
    "        self.p1 = nn.MaxPool2d(2)\n",
    "        self.e2 = nn.Sequential(ConvBNReLU(base, base*2),    ConvBNReLU(base*2, base*2))\n",
    "        self.p2 = nn.MaxPool2d(2)\n",
    "        self.e3 = nn.Sequential(ConvBNReLU(base*2, base*4),  ConvBNReLU(base*4, base*4))\n",
    "        self.p3 = nn.MaxPool2d(2)\n",
    "        self.e4 = nn.Sequential(ConvBNReLU(base*4, base*8),  ConvBNReLU(base*8, base*8))\n",
    "        self.p4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bott = nn.Sequential(ConvBNReLU(base*8, base*16), ConvBNReLU(base*16, base*16))\n",
    "\n",
    "        # Decoder (no skip connections)\n",
    "        self.d4 = nn.Sequential(ConvBNReLU(base*16, base*8),  ConvBNReLU(base*8, base*8))\n",
    "        self.d3 = nn.Sequential(ConvBNReLU(base*8,  base*4),  ConvBNReLU(base*4, base*4))\n",
    "        self.d2 = nn.Sequential(ConvBNReLU(base*4,  base*2),  ConvBNReLU(base*2, base*2))\n",
    "        self.d1 = nn.Sequential(ConvBNReLU(base*2,  base),    ConvBNReLU(base,   base))\n",
    "\n",
    "        self.outc = nn.Conv2d(base, out_ch, kernel_size=1)\n",
    "\n",
    "    def up(self, x, scale=2):  # bilinear upsample\n",
    "        return F.interpolate(x, scale_factor=scale, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        x1 = self.e1(x); x = self.p1(x1)\n",
    "        x2 = self.e2(x); x = self.p2(x2)\n",
    "        x3 = self.e3(x); x = self.p3(x3)\n",
    "        x4 = self.e4(x); x = self.p4(x4)\n",
    "\n",
    "        # Bottleneck\n",
    "        x  = self.bott(x)\n",
    "\n",
    "        # Decode (no concatenations)\n",
    "        x = self.up(x);  x = self.d4(x)\n",
    "        x = self.up(x);  x = self.d3(x)\n",
    "        x = self.up(x);  x = self.d2(x)\n",
    "        x = self.up(x);  x = self.d1(x)\n",
    "\n",
    "        return self.outc(x)  # logits (B, C, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f3ff0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder-Decoder params: 7.07 M\n"
     ]
    }
   ],
   "source": [
    "in_channels = 4\n",
    "num_classes = 4\n",
    "\n",
    "encdec = EncoderDecoder2D(in_ch=in_channels, out_ch=num_classes).to(DEVICE)\n",
    "print(f\"Encoder-Decoder params: {sum(p.numel() for p in encdec.parameters())/1e6:.2f} M\")\n",
    "\n",
    "\n",
    "LR_ENCDEC = LR\n",
    "optimizer_ed = torch.optim.Adam(encdec.parameters(), lr=LR_ENCDEC)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler_ed = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer_ed,\n",
    "    max_lr=LR_ENCDEC,            # max learning rate\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    pct_start=0.1,               #  10% of cycle is warm-up\n",
    "    div_factor=1e3,              # initial LR = max_lr / div_factor\n",
    "    final_div_factor=1e4,        # min LR = initial LR / final_div_factor\n",
    "    anneal_strategy='cos',       # cosine annealing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abfeac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanhuoyuan_gmail_com/conda/envs/dml/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EncDec 001] train=1.1290  val=0.9962  dice=0.0365\n",
      "[EarlyStopping] best min: 0.996158\n",
      "[EncDec 002] train=0.9222  val=0.9168  dice=0.0578\n",
      "[EarlyStopping] best min: 0.916815\n",
      "[EncDec 003] train=0.8479  val=0.9564  dice=0.0799\n",
      "[EarlyStopping] no improvement (1/10)\n",
      "[EncDec 004] train=0.7850  val=0.7447  dice=0.1321\n",
      "[EarlyStopping] best min: 0.744729\n",
      "[EncDec 005] train=0.7038  val=0.6662  dice=0.1564\n",
      "[EarlyStopping] best min: 0.666156\n",
      "[EncDec 006] train=0.6242  val=0.5979  dice=0.1682\n",
      "[EarlyStopping] best min: 0.597857\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m scaler_ed = torch.cuda.amp.GradScaler(enabled=use_amp)\n\u001b[32m      3\u001b[39m es_ed = EarlyStopping(mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, patience=ES_PATIENCE, min_delta=ES_MIN_DELTA,\n\u001b[32m      4\u001b[39m                    ckpt_path=OUTPUT_DIR/\u001b[33m'\u001b[39m\u001b[33mencdec_best.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m ed_history = \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencdec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_ed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_ed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_step\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mes_ed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mencdec_best.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mencdec_training_history.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEncDec\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# # load ed_history from file\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# with open(OUTPUT_DIR/'encdec_training_history.json', 'r') as f:\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#     ed_history = json.load(f)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mfit\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, device, epochs, scheduler, scheduler_step, use_amp, early_stopping, ckpt_path, history_path, num_classes, model_name)\u001b[39m\n\u001b[32m    104\u001b[39m history = {\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mval_dice\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     tr = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_amp\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     va, vd = validate(model, val_loader, criterion, device,\n\u001b[32m    112\u001b[39m                       num_classes=num_classes, use_amp=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# ReduceLROnPlateau: step on val metric\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device, scaler, scheduler, scheduler_step, use_amp)\u001b[39m\n\u001b[32m     50\u001b[39m total, n = \u001b[32m0.0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m     51\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/dml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    634\u001b[39m         \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    635\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/dml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/dml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1287\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/dml/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1121\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1130\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1131\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1132\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1134\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1135\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1136\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1137\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/dml/lib/python3.11/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/conda/envs/dml/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "use_amp = (DEVICE == \"cuda\")\n",
    "scaler_ed = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "es_ed = EarlyStopping(mode='min', patience=ES_PATIENCE, min_delta=ES_MIN_DELTA,\n",
    "                   ckpt_path=OUTPUT_DIR/'encdec_best.pt')\n",
    "\n",
    "ed_history = fit(\n",
    "    model=encdec,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    criterion=criterion, optimizer=optimizer_ed, device=DEVICE, epochs=EPOCHS,\n",
    "    scheduler=scheduler_ed, scheduler_step='batch',\n",
    "    use_amp=(DEVICE.type=='cuda'),\n",
    "    early_stopping=es_ed,\n",
    "    ckpt_path=OUTPUT_DIR/'encdec_best.pt',\n",
    "    history_path=OUTPUT_DIR/'encdec_training_history.json',\n",
    "    model_name=\"EncDec\", num_classes=4\n",
    ")\n",
    "\n",
    "# # load ed_history from file\n",
    "# with open(OUTPUT_DIR/'encdec_training_history.json', 'r') as f:\n",
    "#     ed_history = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690efd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIR/'encdec_training_history.json', 'w') as f:\n",
    "#     json.dump(ed_history, f)\n",
    "\n",
    "plt.figure(figsize=(6,4)); plt.plot(ed_history['epoch'], ed_history['train_loss'], label='train')\n",
    "plt.plot(ed_history['epoch'], ed_history['val_loss'], label='val'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Loss Curves'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4)); plt.plot(ed_history['epoch'], ed_history['val_dice'], label='val_dice')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Dice'); plt.title('Validation Dice'); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encdec.load_state_dict(torch.load(OUTPUT_DIR/'encdec_best.pt', map_location=DEVICE))\n",
    "encdec.eval()\n",
    "# viz_overlays(val_ds, encdec, device=DEVICE, n=6, base_channel=1, seed=35)\n",
    "viz_overlays_pro(val_ds, encdec, device=DEVICE, n=6, samples_per_row=2, show_error=True, base_channel=1, seed=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_slice,   ed_vol,   ed_overall = evaluate_multiclass(\n",
    "    test_loader, encdec, device=DEVICE, add_overall_mean=True, save_csv_dir=OUTPUT_DIR, model_name=\"encdec\"\n",
    ")\n",
    "print(\"Overall mean row:\")\n",
    "print(ed_overall)          # {'dice_Necrosis':..., 'dice_Edema':..., ..., 'volume_id':'Overall_Mean'}\n",
    "display(ed_vol.tail(3))  # Overall_Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_cuda(model=encdec, optimizer=optimizer_ed, scheduler=scheduler_ed, scaler=scaler_ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed3db1",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train 2D U-Net\n",
    "### 3.1 Train 2D U-Net\n",
    "\n",
    "1. Train a small 2D U-Net (4-channel input → 1-channel tumor mask)\n",
    "2. Plot training curves & qualitative overlays\n",
    "3. Evaluate on test set: Dice / IoU / 2D-HD95 (per-slice) and aggregate per-volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1bba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, in_ch=4, out_ch=1, base=32):\n",
    "        super().__init__()\n",
    "        self.d1 = DoubleConv(in_ch, base);  self.p1 = nn.MaxPool2d(2)\n",
    "        self.d2 = DoubleConv(base, base*2); self.p2 = nn.MaxPool2d(2)\n",
    "        self.d3 = DoubleConv(base*2, base*4); self.p3 = nn.MaxPool2d(2)\n",
    "        self.d4 = DoubleConv(base*4, base*8)\n",
    "        self.u3 = nn.ConvTranspose2d(base*8, base*4, 2, stride=2); self.ud3 = DoubleConv(base*8, base*4)\n",
    "        self.u2 = nn.ConvTranspose2d(base*4, base*2, 2, stride=2); self.ud2 = DoubleConv(base*4, base*2)\n",
    "        self.u1 = nn.ConvTranspose2d(base*2, base, 2, stride=2);   self.ud1 = DoubleConv(base*2, base)\n",
    "        self.outc = nn.Conv2d(base, out_ch, 1)\n",
    "    def forward(self, x):\n",
    "        x1 = self.d1(x)\n",
    "        x2 = self.d2(self.p1(x1))\n",
    "        x3 = self.d3(self.p2(x2))\n",
    "        x4 = self.d4(self.p3(x3))\n",
    "        x = self.u3(x4); x = self.ud3(torch.cat([x, x3], dim=1))\n",
    "        x = self.u2(x);  x = self.ud2(torch.cat([x, x2], dim=1))\n",
    "        x = self.u1(x);  x = self.ud1(torch.cat([x, x1], dim=1))\n",
    "        return self.outc(x)\n",
    "\n",
    "model = UNet2D(in_ch=4, out_ch=4).to(DEVICE)\n",
    "print(f'Model params: {sum(p.numel() for p in model.parameters())/1e6:.2f} M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08116b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = {'epoch':[], 'train_loss':[], 'val_loss':[], 'val_dice':[]}\n",
    "use_amp = (DEVICE == \"cuda\")\n",
    "scaler_unet = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "optimizer_unet = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "steps_per_epoch = len(train_loader)\n",
    "scheduler_unet = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer_unet,\n",
    "    max_lr=LR,            # max learning rate\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    pct_start=0.1,               \n",
    "    anneal_strategy='cos',\n",
    ")\n",
    "\n",
    "\n",
    "es_unet = EarlyStopping(mode='min', patience=ES_PATIENCE, min_delta=ES_MIN_DELTA,\n",
    "                   ckpt_path=OUTPUT_DIR/'unet_best.pt')\n",
    "\n",
    "hist_unet = fit(\n",
    "    model=model,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    criterion=criterion, optimizer=optimizer_unet, device=DEVICE, epochs=EPOCHS,\n",
    "    scheduler=scheduler_unet, scheduler_step='batch',\n",
    "    use_amp=(DEVICE.type=='cuda'),\n",
    "    early_stopping=es_unet,\n",
    "    ckpt_path=OUTPUT_DIR/'unet_best.pt',\n",
    "    history_path=OUTPUT_DIR/'unet2d_training_history.json',\n",
    "    model_name=\"UNet\", num_classes=4\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34e341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load history from file\n",
    "# with open(OUTPUT_DIR/'unet2d_training_history.json', 'r') as f:\n",
    "#     hist_unet = json.load(f)\n",
    "\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist_unet['epoch'], hist_unet['train_loss'], label='train')\n",
    "plt.plot(hist_unet['epoch'], hist_unet['val_loss'], label='val'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Loss Curves'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist_unet['epoch'], hist_unet['val_dice'], label='val_dice')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Dice'); plt.title('Validation Dice'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(OUTPUT_DIR/'unet2d_best.pt', map_location=DEVICE))\n",
    "model.eval()\n",
    "# viz_overlays(val_ds, model, device=DEVICE, n=6, base_channel=1, seed=35)\n",
    "viz_overlays_pro(val_ds, model, device=DEVICE, n=6, samples_per_row=2, show_error=True, base_channel=1, seed=35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_slice, unet_vol, unet_overall = evaluate_multiclass(\n",
    "    test_loader, model, device=DEVICE, add_overall_mean=True, save_csv_dir=OUTPUT_DIR, model_name=\"unet\"\n",
    ")\n",
    "\n",
    "print(\"Overall mean row:\")\n",
    "print(unet_overall)          # {'dice_Necrosis':..., 'dice_Edema':..., ..., 'volume_id':'Overall_Mean'}\n",
    "display(unet_vol.tail(3))  # Overall_Mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b45eb2",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2 Compare with Endode-Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# with open(OUTPUT_DIR/'unet2d_training_history.json', 'r') as f:\n",
    "#     hist_unet = json.load(f)\n",
    "with open(OUTPUT_DIR/'encdec_training_history.json', 'r') as f:\n",
    "    ed_history = json.load(f)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(hist_unet[\"epoch\"], hist_unet[\"val_loss\"], label=\"UNet val loss\")\n",
    "plt.plot(ed_history[\"epoch\"], ed_history[\"val_loss\"], label=\"EncDec val loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Validation Loss\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.plot(hist_unet[\"epoch\"], hist_unet[\"val_dice\"], label=\"UNet val dice\")\n",
    "plt.plot(ed_history[\"epoch\"], ed_history[\"val_dice\"], label=\"EncDec val dice\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Foreground Dice\"); plt.title(\"Validation Dice\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca444511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load UNet\n",
    "# model = UNet2D(in_ch=in_channels, out_ch=num_classes).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(OUTPUT_DIR/\"unet2d_best.pt\", map_location=DEVICE))\n",
    "# model.eval()\n",
    "\n",
    "# Load best Encoder–Decoder\n",
    "encdec = EncoderDecoder2D(in_ch=in_channels, out_ch=num_classes).to(DEVICE)\n",
    "encdec.load_state_dict(torch.load(OUTPUT_DIR/\"encdec_best.pt\", map_location=DEVICE))\n",
    "encdec.eval()\n",
    "\n",
    "# # Evaluate\n",
    "# unet_slice, unet_vol, unet_overall = evaluate_multiclass(test_loader, model, device=DEVICE, model_name=\"unet\")\n",
    "ed_slice,   ed_vol,   ed_overall   = evaluate_multiclass(test_loader, encdec,   device=DEVICE, model_name=\"encdec\")\n",
    "\n",
    "# Merge per-volume tables and append overall mean row\n",
    "import pandas as pd\n",
    "cmp = unet_vol.merge(ed_vol, on=\"volume_id\", suffixes=(\"_unet\",\"_encdec\"))\n",
    "\n",
    "if unet_overall and ed_overall:\n",
    "    row = {\"volume_id\":\"Overall_Mean\"}\n",
    "    for k,v in unet_overall.items():\n",
    "        if k!=\"volume_id\": row[f\"{k}_unet\"] = v\n",
    "    for k,v in ed_overall.items():\n",
    "        if k!=\"volume_id\": row[f\"{k}_encdec\"] = v\n",
    "    cmp = pd.concat([cmp, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "cmp.to_csv(OUTPUT_DIR/\"compare_unet_vs_encdec.csv\", index=False)\n",
    "display(cmp.iloc[[-1], 9:18])\n",
    "display(cmp.iloc[[-1], 1:9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50929d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_compare(test_ds, encdec, model, device=DEVICE, n=6, model1_name=\"encdec\", model2_name=\"UNet\")\n",
    "viz_compare_diagnostic(\n",
    "    test_ds,\n",
    "    model_1=encdec,\n",
    "    model_2=model,\n",
    "    device=DEVICE,\n",
    "    n=6,\n",
    "    samples_per_row=1,\n",
    "    base_channel=1,\n",
    "    show_overlay=True,\n",
    "    show_boundaries=True,\n",
    "    show_agreement=True,\n",
    "    model_1_name=\"encdec\",\n",
    "    model_2_name=\"UNet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb287ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_cuda(model=model, optimizer=optimizer_unet, scheduler=scheduler_unet, scaler=scaler_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78fa57",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Fine-tune Transfer-learning UNet encoder\n",
    "### 4.1 Fine-tune Transfer-learning UNet encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5375fa4",
   "metadata": {},
   "source": [
    "Build the Transfer-Learning U-Net (ImageNet encoder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c81e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "# os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# print(\"Downloading pretrained encoder weights from Hugging Face Hub...\")\n",
    "# snapshot_download(\"smp-hub/resnet34.imagenet\",\n",
    "#                   allow_patterns=[\"*.safetensors\",\"*.json\"])\n",
    "\n",
    "# os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# print(\"Loading U-Net with pretrained encoder...\")\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "# Define number of input channels (modalities) and output classes\n",
    "in_channels = 4   # T1, T1Gd, T2, FLAIR\n",
    "num_classes = 4   # Background + 3 tumor subregions\n",
    "\n",
    "# Create a U-Net with an ImageNet-pretrained encoder\n",
    "tl_model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",           # Common choices: resnet18/34/50, efficientnet-b0/b3, etc.\n",
    "    encoder_weights=\"imagenet\",        # Use ImageNet pretrained weights\n",
    "    in_channels=in_channels,           # Four MRI modalities\n",
    "    classes=num_classes,               # Four segmentation classes\n",
    "    activation=None,                   # Output logits (for CE + SoftDice)\n",
    ").to(DEVICE)\n",
    "\n",
    "# Freeze encoder layers during warm-up stage\n",
    "def freeze_encoder(m, flag=True):\n",
    "    for p in m.encoder.parameters():\n",
    "        p.requires_grad = not flag\n",
    "\n",
    "print(f\"Transfer Learning Model params: {sum(p.numel() for p in tl_model.parameters())/1e6:.2f} M\")\n",
    "# Freeze entire encoder for warm-up\n",
    "freeze_encoder(tl_model, flag=True)\n",
    "print(f\"Freeze fiinished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace1617",
   "metadata": {},
   "source": [
    "Define optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a02662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-phase training: warm-up (decoder only) -> fine-tune (full model)\n",
    "LR_WARM = LR * 0.5\n",
    "LR_FT = LR_WARM * 0.5\n",
    "TF_EPOCHS = EPOCHS  # total TL epochs\n",
    "WARMUP_EPOCHS = max(1,  int(TF_EPOCHS * 0.1))\n",
    "FT_EPOCHS     = max(1, TF_EPOCHS - WARMUP_EPOCHS)\n",
    "print(f\"TL total epochs: {TF_EPOCHS}  (warm-up: {WARMUP_EPOCHS}, fine-tune: {FT_EPOCHS})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e676c685",
   "metadata": {},
   "source": [
    "Stage-wise training: freeze and unfreeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130566b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def freeze_encoder(m, flag: bool = True):\n",
    "    \"\"\"Freeze/unfreeze encoder params of a segmentation_models_pytorch.Unet.\"\"\"\n",
    "    for p in m.encoder.parameters():\n",
    "        p.requires_grad = not (flag)\n",
    "\n",
    "# --- Stage 1: warm-up decoder only (encoder frozen) --------------------------\n",
    "freeze_encoder(tl_model, True)                       # freeze encoder\n",
    "optim_warm = torch.optim.Adam(tl_model.parameters(), lr=LR_WARM)\n",
    "\n",
    "# per-batch scheduler for short warm-up\n",
    "sched_warm = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optim_warm, max_lr=LR_WARM,\n",
    "    steps_per_epoch=len(train_loader), epochs=WARMUP_EPOCHS,\n",
    "    pct_start=0.1, anneal_strategy=\"cos\"\n",
    ")\n",
    "\n",
    "\n",
    "es_warm = EarlyStopping(\n",
    "    mode='min', patience=ES_PATIENCE, min_delta=ES_MIN_DELTA,\n",
    "    ckpt_path=OUTPUT_DIR / \"tl_warm_best.pt\"\n",
    ")\n",
    "\n",
    "hist_tl_warm = fit(\n",
    "    model=tl_model,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    criterion=criterion, optimizer=optim_warm, device=DEVICE,\n",
    "    epochs=WARMUP_EPOCHS,\n",
    "    scheduler=sched_warm, scheduler_step='batch',   # step every batch\n",
    "    use_amp=(DEVICE.type == 'cuda'),\n",
    "    early_stopping=es_warm,\n",
    "    ckpt_path=OUTPUT_DIR / \"tl_warm_best.pt\",\n",
    "    history_path=OUTPUT_DIR / \"tl_warm_history.json\",\n",
    "    model_name=\"TL-UNet (warm)\",\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "# --- Stage 2: fine-tune all layers (unfreeze encoder) ------------------------\n",
    "freeze_encoder(tl_model, False)                      # unfreeze encoder\n",
    "\n",
    "# re-create optimizer after unfreezing so it sees all params\n",
    "optim_ft = torch.optim.Adam(tl_model.parameters(), lr=LR_FT)\n",
    "\n",
    "# Can use OneCycleLR (batch) OR StepLR (epoch) OR ReduceLROnPlateau (plateau)\n",
    "sched_ft = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optim_ft, max_lr=LR_FT,\n",
    "    steps_per_epoch=len(train_loader), epochs=FT_EPOCHS,\n",
    "    pct_start=0.1, anneal_strategy=\"cos\"\n",
    ")\n",
    "sched_step = 'batch'\n",
    "\n",
    "# ReduceLROnPlateau on val_loss\n",
    "# sched_ft = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optim_ft, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "# )\n",
    "# sched_step = 'plateau'\n",
    "\n",
    "es_ft = EarlyStopping(\n",
    "    mode='min', patience=ES_PATIENCE, min_delta=ES_MIN_DELTA,\n",
    "    ckpt_path=OUTPUT_DIR / \"tl_best.pt\"\n",
    ")\n",
    "\n",
    "hist_tl_ft = fit(\n",
    "    model=tl_model,\n",
    "    train_loader=train_loader, val_loader=val_loader,\n",
    "    criterion=criterion, optimizer=optim_ft, device=DEVICE,\n",
    "    epochs=FT_EPOCHS,\n",
    "    scheduler=sched_ft, scheduler_step=sched_step,\n",
    "    use_amp=(DEVICE.type == 'cuda'),\n",
    "    early_stopping=es_ft,\n",
    "    ckpt_path=OUTPUT_DIR / \"tl_best.pt\",\n",
    "    history_path=OUTPUT_DIR / \"tl_ft_history.json\",\n",
    "    model_name=\"TL-UNet (finetune)\",\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "# --- merge histories for plotting ---------------------------------\n",
    "import json\n",
    "hist_tl = {\n",
    "    \"epoch\":              hist_tl_warm[\"epoch\"] + [e + WARMUP_EPOCHS for e in hist_tl_ft[\"epoch\"]],\n",
    "    \"train_loss\":         hist_tl_warm[\"train_loss\"] + hist_tl_ft[\"train_loss\"],\n",
    "    \"val_loss\":           hist_tl_warm[\"val_loss\"] + hist_tl_ft[\"val_loss\"],\n",
    "    \"val_dice\":           hist_tl_warm[\"val_dice\"] + hist_tl_ft[\"val_dice\"],\n",
    "}\n",
    "with open(OUTPUT_DIR / \"tl_training_history_merged.json\", \"w\") as f:\n",
    "    json.dump(hist_tl, f)\n",
    "print(\"TL-UNet done. Best weights at:\", OUTPUT_DIR / \"tl_best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load history from file\n",
    "# with open(OUTPUT_DIR/'tl_training_history_merged.json', 'r') as f:\n",
    "#     history = json.load(f)\n",
    "\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist_tl['epoch'], hist_tl['train_loss'], label='train')\n",
    "plt.plot(hist_tl['epoch'], hist_tl['val_loss'], label='val'); plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
    "plt.title('Loss Curves'); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4)); plt.plot(hist_tl['epoch'], hist_tl['val_dice'], label='val_dice')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Dice'); plt.title('Validation Dice'); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8986a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tl_model.load_state_dict(torch.load(OUTPUT_DIR/'tl_best.pt', map_location=DEVICE))\n",
    "tl_model.eval()\n",
    "# viz_overlays_pro(val_ds, tl_model, device=DEVICE, n=6, samples_per_row=3, base_channel=1, seed=35)\n",
    "viz_overlays_pro(val_ds, tl_model, device=DEVICE, n=6, samples_per_row=2, show_error=True, base_channel=1, seed=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33123863",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_slice, tl_vol, tl_overall = evaluate_multiclass(\n",
    "    test_loader, tl_model, device=DEVICE, add_overall_mean=True, save_csv_dir=OUTPUT_DIR, model_name=\"tl_unet\"\n",
    ")\n",
    "\n",
    "print(\"Overall mean row:\")\n",
    "print(tl_overall)          # {'dice_Necrosis':..., 'dice_Edema':..., ..., 'volume_id':'Overall_Mean'}\n",
    "display(tl_vol.tail(3))  # Overall_Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce029a2",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2 Compare with U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f6787",
   "metadata": {},
   "source": [
    "Quantitative evaluation and comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e7cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(OUTPUT_DIR/'unet2d_training_history.json', 'r') as f:\n",
    "    hist_unet = json.load(f)\n",
    "# with open(OUTPUT_DIR/'tl_training_history.json', 'r') as f:\n",
    "#     hist_tl = json.load(f)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_unet[\"epoch\"], hist_unet[\"val_loss\"], label=\"UNet val loss\")\n",
    "plt.plot(hist_tl[\"epoch\"], hist_tl[\"val_loss\"], label=\"Transfer val loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Validation Loss\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(hist_unet[\"epoch\"], hist_unet[\"val_dice\"], label=\"UNet val dice\")\n",
    "plt.plot(hist_tl[\"epoch\"], hist_tl[\"val_dice\"], label=\"Transfer val dice\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Foreground Dice\"); plt.title(\"Validation Dice\"); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9f006",
   "metadata": {},
   "source": [
    "Qualitative visualization (UNet vs Transfer vs Ground Truth):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1061ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNet model\n",
    "model = UNet2D(in_ch=in_channels, out_ch=num_classes).to(DEVICE)\n",
    "model.load_state_dict(torch.load(OUTPUT_DIR/\"unet_best.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "unet_slice, unet_vol, unet_overall = evaluate_multiclass(test_loader, model, device=DEVICE, model_name=\"unet\")\n",
    "\n",
    "# # Transfer model\n",
    "# tl_model = smp.Unet(\n",
    "#     encoder_name=\"resnet34\",           # Common choices: resnet18/34/50, efficientnet-b0/b3, etc.\n",
    "#     encoder_weights=\"imagenet\",        # Use ImageNet pretrained weights\n",
    "#     in_channels=in_channels,           # Four MRI modalities\n",
    "#     classes=num_classes,               # Four segmentation classes\n",
    "#     activation=None,                   # Output logits (for CE + SoftDice)\n",
    "# ).to(DEVICE)\n",
    "# tl_model.load_state_dict(torch.load(OUTPUT_DIR/\"tl_best.pt\", map_location=DEVICE))\n",
    "# tl_model.eval()\n",
    "# tl_slice, tl_vol, tl_overall = evaluate_multiclass(test_loader, tl_model, device=DEVICE, model_name=\"tl_unet\")\n",
    "\n",
    "\n",
    "# Merge and compute per-volume + overall comparison\n",
    "metric_cols = [c for c in tl_vol.columns if c != \"volume_id\"]\n",
    "cmp = unet_vol.merge(tl_vol, on=\"volume_id\", suffixes=(\"_unet\", \"_tl\"))\n",
    "\n",
    "\n",
    "\n",
    "if unet_overall and tl_overall:\n",
    "    row = {\"volume_id\": \"Overall_Mean\"}\n",
    "    for k, v in unet_overall.items():\n",
    "        if k != \"volume_id\": row[f\"{k}_unet\"] = v\n",
    "    for k, v in tl_overall.items():\n",
    "        if k != \"volume_id\": row[f\"{k}_tl\"] = v\n",
    "    import pandas as pd\n",
    "    cmp = pd.concat([cmp, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "cmp.to_csv(OUTPUT_DIR/\"compare_unet_vs_TL.csv\", index=False)\n",
    "display(cmp.iloc[[-1], 9:18])\n",
    "display(cmp.iloc[[-1], 1:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz_compare(test_ds, model, tl_model, device=DEVICE, n=6, model1_name=\"UNet\", model2_name=\"TL\")\n",
    "# viz_compare_triplet(\n",
    "#     test_ds,\n",
    "#     model1=model,            # dashed\n",
    "#     model2=tl_model,               # dotted (or tl_model)\n",
    "#     device=DEVICE,\n",
    "#     n=6,\n",
    "#     samples_per_row=3,\n",
    "#     base_channel=1,              # e.g., T1Gd\n",
    "#     title1=\"UNet\",\n",
    "#     title2=\"TL\"\n",
    "# )\n",
    "\n",
    "viz_compare_diagnostic(\n",
    "    test_ds,\n",
    "    model_1=model,\n",
    "    model_2=tl_model,\n",
    "    device=DEVICE,\n",
    "    n=6,\n",
    "    samples_per_row=1,\n",
    "    base_channel=1,\n",
    "    show_overlay=True,\n",
    "    show_boundaries=True,\n",
    "    show_agreement=True,\n",
    "    model_1_name=\"UNet2D\",\n",
    "    model_2_name=\"TL\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f35c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_cuda(model=tl_model, optimizer=optim_ft, scheduler=sched_warm, scaler=scaler_ed)\n",
    "cleanup_cuda(model=tl_model, optimizer=optim_ft, scheduler=sched_ft, scaler=scaler_ed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15ad34",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Final Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeeac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read from csv\n",
    "cmp_1 = pd.read_csv(OUTPUT_DIR/\"compare_unet_vs_encdec.csv\")\n",
    "\n",
    "display(cmp_1.iloc[[-1], 9:18])\n",
    "display(cmp_1.iloc[[-1], 1:9])\n",
    "\n",
    "cmp_2 = pd.read_csv(OUTPUT_DIR/\"compare_unet_vs_TL.csv\")\n",
    "display(cmp_2.iloc[[-1], 9:18])\n",
    "# display(cmp_2.iloc[[-1], 1:9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
